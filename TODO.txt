BACKLOG:
* Fix bilateral filters by not normalizing between passes.
* CMake: use the native CUDA compilation. https://cmake.org/cmake/help/v3.12/module/FindCUDA.html
** https://devblogs.nvidia.com/building-cuda-applications-cmake/
* Roughness regularization. Perhaps just by clamping roughness along a path and then relax pr iteration.
** Base on some (pdf, cos_theta) -> roughness.
** Base factor and decay rate as parameters
* Progressive multijittered sample sequences
** Unit test for different sampling scenarios.
** Debug VeachScene sampling with no next event estimation. The large light's reflection on the second slab is very noisy around the edges.
** Path debugger. Select pixel and trace all 4096 paths, then visualize ranges of paths.
** Heitz and Georgiev blue noise screen space seed scheme. Optimize towards equal distribution of first four seeds. Seed, swap row/columns and then optimize random cells.
*** Toroidal shift + random sample index?
*** Toroidal shift using 14 bit bitmask, right-shift by 1 pr dimension and reinsert the swapped bit as the most significant one.
** Pregenerate samples with 64 bluenoise samples for OptiX.
* Film grain
** http://www.loopit.dk/banding_in_games.pdf
** https://www.shadertoy.com/view/4t2SDh
* Decima Engine Area lights
** https://github.com/EpicGames/UnrealEngine/blob/release/Engine/Shaders/Private/BRDF.ush
** https://www.guerrilla-games.com/read/decima-engine-advances-in-lighting-and-aa
* OptiX (filtering) backends
** Filter neighbouring light samples before reconstruction - Screen space photon mapping approach with MIS weights / Path filtering.
*** Output buffers: Material ID, texcoord, position, normal, ray dir, weight, light sample, indirect sample. Define for compressing directions and use half/byte for some values.
*** Output screen space sample based on path/BSDF PDF.
**** Perhaps select a vertex if the BSDF PDF is less than 1 / PI or maybe even a bit more. Then vertices are selected roughly proportional to how 'diffuse' they are.
**** Trace ray footprint by assuming the ray is a cone. Then use Distributions::Cone::CosTheta(float angle) to compute the angle of each new cone after an intersection and use that for the density estimation kernel bandwidth.
** Pass BSDF and light sample to the path tracing RGP and let it handle accumulated radiance.
* SDF shadows - check UE4
** http://kunzhou.net/2013/fur-rendering-tvcg.pdf
* SSAO
** Intensity should be relative to sample distance, not absolute distance intensity.
*** When hitting the screen space max radius then results should be independent of the world radius. Are they? Test scene with three 'infinite' walls.
** Try seeding with hilbert curve + hash from 'Stratified sampling for stochastic transparency'
* SSBN
** Bent normal
*** Path tracer backend (Also one for SSAO?)
*** For diffuse env map lookup.
*** For cones compute aparture from visibility
*** Intersect bent cone with light cone or distribution and bake into 3D LUT (GTSO)
* Add coat to the default material.
** http://jcgt.org/published/0003/04/03/paper-lowres.pdf and https://www.youtube.com/watch?v=4nKb9hRYbPA
** Or just add a GGX layer on top with an iridescence parameter (red and blue fresnel offset relative to green. [Specularity, iridescence)
* Normals revisited
** Offset slightly along the geometric normal as well? To avoid self shadowing and hard edges on tesselated balls.
*** Or perhaps offset along the incoming direction? That one should obviously not intersect anything else.
** Bump mapping
*** Filter bumpmap mipmaps using Toksvig05, SGGX or AGAA NDF construction.
*** Compute AO maps based on normal maps.
*** Warp the hemisphere by the shading normal, such that samples are always drawn in an unwarped hemisphere, but then warped such that <0,0,1> corresponds to the shading/bump normal direction and samples halfway between <0,0,1> and the border of the hemisphere will be warped to be halfway between the shading normal and hemisphere.
*** 'Fix' the normals at edges. Let the shading normal lerp towards the geometric normal at grazing angles. (Offset normal by -view_dir until the dot product is 0.)
*** Or warp the hemisphere by the shading normal, such that the hemisphere is turned into a cone that only grazes the surface. That way samples would need to be converted from hemisphere to cone and back again. Pro: No special casing on light / view direction. Con: Black surface when view dir is outside the cone.
** Perhaps just dim 'blocked' regions as they could be lit indirectly.
* BTDF
** Transmission factor. Use IOR computed from specularity.

libs
* TinyDNN
** Cat detection
** A neural algorithm of artistic style, Gatys et al, 2015
** Face recognition.
** Use a siamese network to detect individual predefined lego bricks. (As opposed to classifying all lego bricks for all pixels.) Perhaps use transfer learning on some face recognition net
* ReactPhysics or BulletPhyssics (or both)
* RtAudio wrapper - https://github.com/thestk/rtaudio
* LuaJit - OpenSource (Fallback to interpretted Lua if the target architecture isn't supported.)